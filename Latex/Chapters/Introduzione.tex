%************************************************
\chapter{Introduzione}\label{ch:introduzione}
%************************************************

% L'abilità di riconoscere modelli da parte delle macchine è una cosa molto bella. 
% Per aiutare i computer dei nostri giorni che non ce la fanno più, si stanno ideando 
% varie soluzioni. Qui parlo dei computer quantistici, che sfruttano proprietà 
% della materia quando è molto fredda, che ci permettono di fare cose strabilianti. 

% L'articolo con cui è partita la mia attività di tesi è quello di Tacchino, 
% in cui implementa un percettrone a due qubit, che è capace di riconoscere 
% motivi a quattro bit. Quello su cui mi concentro io è un algoritmo analogo 
% e ugualmente semplice, detto k-Nearest Neighbour, ovvero i k vicini più vicini. 

% Per affrontare l'argomento sono partito dalla tesi di Mark Fingerhuth, che 
% ha implementato un algoritmo ma non è riuscito a farlo girare su un vero 
% computer quantistico, perché non ce n'erano ancora di abbastanza potenti. 
% Adesso ci sono, e ci divertiamo. 

%****************************************

L'abilità di riconoscere facce familiari, di comprendere il linguaggio parlato e di 
distinguere tra tipi di piante viene naturale agli esseri umani, sebbene
% sebbene, benché, quantunque, anche se
questi processi di riconoscimento di modelli e di classificazione siano intrinsecamente 
complessi. Il \ac{ML}, una sottodisciplina dell'intelligenza artificiale, 
si occupa dello sviluppo di algoritmi che eseguono questo tipo di compiti, permettendo 
in questo modo % così
ai computer di trovare e riconoscere modelli nei dati e di classificare input 
% dati da elaborare
sconosciuti % ignoti
basandosi sui % in base ai
precedenti dati di addestramento. % allenamento, ammaestramento
Tali algoritmi formano il nucleo di p.e. motori di riconoscimento del linguaggio % parola
umano e di raccomandazione come quelli usati da Amazon. % Fonti

Secondo l'IBM, approssimativamente % circa
2,5 quintilioni ($10^{18}$) byte di dati sono creati ogni giorno. 
Questo numero crescente implica che molte aree che hanno a che fare con i dati 
avranno bisogno prima o poi di algoritmi avanzati che possano dare un senso % significato
al contenuto dei dati, recuperare modelli e rivelare correlazioni. 
In qualsiasi modo, il più % la maggior parte
degli algoritmi di \ac{ML} implica l'esecuzione di operazioni computazionalmente dispendiose 
e fare ciò su grandi insiemi di dati richiede % porta via, impiega
inevitabilmente molto % un sacco di
tempo. % Fonti
Di conseguenza, % così, perciò
diventa sempre più importante trovare metodi % modi, maniere
efficienti di trattare % manipolare, avere a che fare con, gestire
grandi dati. 

Una soluzione promettente è l'uso della computazione quantistica, che è stata 
oggetto di intensa ricerca nelle ultime decadi. Un \ac{CQ} usa 
sistemi quantomeccanici e le loro proprietà uniche per manipolare e processare 
l'informazione in modi impossibili per i computer classici. 
Così come un computer classico manipola bit, un computer quantistico fa uso dei 
cosiddetti bit quantistici (qubit). I bit e i qubit sono entrambi entità binarie, 
il che significa che possono assumere solo i valori 0 o 1. Ad ogni modo, un bit 
classico non probabilistico può assumere solo uno di questi valori alla volta, 
laddove un qubit può anche trovarsi in una sovrapposizione lineare dei due stati. 

Questa proprietà dà luogo al parallelismo quantistico, il quale permette l'esecuzione 
di determinate operazioni su vari stati quantistici allo stesso tempo. 
Nonostante questo vantaggio, la difficoltà nella computazione quantistica risiede 
nel recupero della soluzione computata, giacché la misura di un qubit lo fa 
collassare in un singolo bit classico e in tal modo distrugge l'informazione riguardo 
la sua precedente sovrapposizione. Diversi algoritmi quantistici sono stati proposti, 
che forniscono accelerazioni esponenziali se confrontati con le loro controparti classiche, 
con l'algoritmo di fattorizzazione in numeri primi di Shor tra i più famosi. % Fonti
Così, la computazione quantistica ha il potenziale di migliorare la potenza computazionale, 
accelerare l'elaborazione di grandi dati e possibilmente risolvere alcuni problemi che sono 
praticamente irrisolvibili su computer classici, p.e. problemi di ottimizzazione 
computazionalmente esaustivi come il ben noto problema del commesso viaggiatore con più 
di 1000 città. % Fonti

Tenendo in considerazione questi vantaggi, la combinazione di computazione quantistica e \ac{ML}
classico nel nuovo campo del \ac{MLQ} sembra quasi naturale. Più nello specifico, si parla di 
machine learning aumentato quantisticamente quando la computazione quantistica è usata per 
migliorare algoritmi di \ac{ML} classico. Cionondimeno, entrambi i campi sono mutualisticamente 
benefici l'un l'altro, poiché il \ac{ML} classico può anche essere usato per migliorare aspetti 
della computazione quantistica. Per esempio, Las Heras, Alvarez-Rodriguez, Solano e Sanz % Fonti
hanno mostrato che un algoritmo genetico classico può essere usato per ridurre l'errore sperimentale 
nelle porte logiche quantistiche. Comunque, questa tesi si concentrerà solo nel campo del machine 
learning aumentato quantisticamente. 

\section{Motivazione}

Il \ac{ML} classico è un ambito molto concreto poiché può essere direttamente testato, 
verificato e implementato su qualsiasi computer classico commerciale. 
Fino ad ora, il \ac{MLQ} è stato per la gran parte di natura teorica, dato che le risorse 
computazionali richieste sono solo parzialmente sviluppate. Per produrre soluzioni affidabili, 
gli algoritmi di \ac{MLQ} spesso richiedono un numero enorme di qubit con correzione 
degli errori e qualche sistema di memorizzazione di dati quantistici, per esempio la 
\ac{QRAM} proposta da Giovannetti, Lloyd e Maccone. % Fonti
% Io ho usato quella proposta da Petruccione, devo vedere se inserire il riferimento al posto 
% di questo o introdurlo dopo nel discorso
In ogni caso, al giorno d'oggi il massimo numero di qubit p.e. a superconduzione, da quanto 
viene riferito, usati per il calcolo sono quattordici; 
% si intende quelli a disposizione pubblica, poi il massimo al mondo bisogna verificarlo
il D-Wave II quantum annealing device fornisce 1152 qubit ma può risolvere solo una ristretta 
classe di problemi, e una \ac{QRAM} dedicata non è stata ancora sviluppata. % Fonti
% Verificare che sia ancora così
Inoltre, la correzione degli errori dei qubit è un campo di ricerca ancora attivo 
e sebbene ci siano stati notevoli miglioramenti, % mettere nota a margine con dati del miglioramento
la maggior parte dei \ac{CQ} preliminari descritti hanno a che fare con qubit senza correzione 
d'errore, con vite brevi e sono, quindi, inadatti per grandi implementazioni di \ac{MLQ}. 

Negli ultimi anni ci sono state varie implementazioni innovative di algoritmi di \ac{MLQ} che 
aprono la strada a nuove possibilità di sperimentazione. 
Tacchino\cite{tacchino} ha implementato un percettrone e Schuld\cite{schuld} ha implementato un 
algoritmo \ac{KNN} su hardware quantistico. 
Li, Liu, Xu e Du hanno distinto con successo cifre scritte a mano usando un quantum support 
vector machine su un banco di prova a risonanza magnetica nucleare con quattro qubit. In più, 
Cai et al. sono stati i primi a dimostrare sperimentalmente il machine learning quantistico su 
un \ac{CQ} fotonico e hanno mostrato che la distanza tra due vettori e il loro prodotto 
interno possono effettivamente essere computati quantomeccanicamente. 
Infine, Ristè et al. hanno risolto un problema di learning parity con cinque qubit 
superconduttivi e hanno trovato che un vantaggio quantistico può già essere osservato 
in sistemi senza correzione degli errori. 

Considerando il distacco tra il numero di algoritmi di \ac{MLQ} e le poche realizzazioni 
sperimentali, considerando inoltre i recenti sviluppi nell'hardware quantistico, diventa 
importante scoprire come i problemi di \ac{MLQ} realizzati di \ac{CQ} di piccola dimensione 
scalino quando implementati su \ac{CQ} con maggiori risorse. 
Di qui, l'obiettivo di questa ricerca è il fornire implementazioni come prova di principio, 
simulazioni e risultati sperimentali di algoritmi di \ac{MLQ} selezionati a partire da 
insiemi di dati di grandezza da piccola a media, osservando come cambiano i requisiti ed 
i risultati passando da un caso all'altro. A questo scopo, l'algoritmo \acf{KNN}, uno tra i 
più semplici algoritmi di \ac{ML}, è stato scelto come buon esempio minimale per 
l'implementazione, la simulazione e la sperimentazione quantistica. 
Questo è un passo necessario nel tentativo di far passare il \ac{MLQ} da un ambito di 
ricerca quasi puramente teorico ad un campo più applicativo come è il \ac{ML} classico. 

\section{Domanda di ricerca}

Alla luce della natura teorica dell'attuale ricerca nel \ac{MLQ} 
e del piccolo numero di realizzazioni sperimentali, questa ricerca si dedicherà 
alla seguente domanda: 
\begin{quote}
    quali sono le opportunità di implementazione pratica e l'efficienza di un algoritmo 
    k-nearest neighbours di machine learning quantistico su computer quantistici di piccola 
    e media scala attualmente disponibili?
\end{quote}
I capitoli seguenti introdurranno le basi teoriche necessarie e gli strumenti usati 
per rispondere a questa domanda di ricerca. 

%*****************************************
%*****************************************
%*****************************************
%*****************************************
%*****************************************
